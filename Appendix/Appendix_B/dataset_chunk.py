# -*- coding: utf-8 -*-
"""dataset_chunk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zmQJ-rClkL8jK9RBthElISOklkUagoxT
"""

import pandas as pd
import gdown

#filmstarts dataset.

reviews_file_id = "17l-LzyTfB_1FlIpQDAlV2a0Yestyu-CI"
labeled_file_id = "12t-EeIEeMq1tdQRkYbP6XFohhdAHIh0o"

reviews_file_path = "reviews.csv"
labeled_file_path = "labels.csv"

gdown.download(f"https://drive.google.com/uc?id={reviews_file_id}", reviews_file_path, quiet=False)
gdown.download(f"https://drive.google.com/uc?id={labeled_file_id}", labeled_file_path, quiet=False)

reviews_df = pd.read_csv(reviews_file_path, sep='\t', header=None, names=['URL', 'Rating', 'Review'], on_bad_lines='skip')
labeled_df = pd.read_csv(labeled_file_path, sep='\t', header=None, names=['Label'])

labeled_df[['Sentiment', 'Score']] = labeled_df['Label'].str.split(' ', n=1, expand=True)
labeled_df['Score'] = pd.to_numeric(labeled_df['Score'], errors='coerce')
reviews_df = reviews_df.drop(columns=['Rating'])
combined_df = pd.concat([reviews_df, labeled_df], axis=1)
combined_df = combined_df.dropna(subset=['Review', 'Score'])
combined_df.head()

#list of modal particles.
MODAL_PARTICLES = ["halt", "eben", "eigentlich", "wirklich", "mal", "ja", "ruhig", "nun einmal", "doch", "schon"]

!python -m spacy download de_core_news_sm

from transformers import AutoTokenizer, AutoModel
import torch
import spacy
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = AutoTokenizer.from_pretrained("bert-base-german-cased")
model = AutoModel.from_pretrained("bert-base-german-cased")

nlp = spacy.load("de_core_news_sm")

def classify_modal_particle_bert_grammar(sentence):
    inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
    hidden_states = outputs.hidden_states[-1]
    doc = nlp(sentence)
    modal_particles = []

    for i, token in enumerate(doc):
        if token.text.lower() not in MODAL_PARTICLES:
            continue

        token_ids = tokenizer.encode(token.text, add_special_tokens=False)
        input_id_list = inputs["input_ids"][0].tolist()
        if token_ids[0] not in input_id_list:
            continue
        token_index = input_id_list.index(token_ids[0])

        start_idx = max(token_index - 2, 0)
        end_idx = min(token_index + 3, hidden_states.shape[1])
        context_vectors = hidden_states[0][start_idx:end_idx]
        if context_vectors.shape[0] == 0:
            continue

        token_vector = hidden_states[0][token_index].unsqueeze(0)
        context_mean = context_vectors.mean(dim=0).unsqueeze(0)
        if torch.isnan(context_mean).any() or torch.isnan(token_vector).any():
            continue

        similarity = cosine_similarity(token_vector, context_mean)[0][0]

        modified_sentence = " ".join([t.text for t in doc if t.i != token.i])
        modified_doc = nlp(modified_sentence)

        if (
            token.pos_ in ["PART", "ADV", "INTJ"]
            and len(list(modified_doc.sents)) == len(list(doc.sents))
            and similarity > 0.5
        ):
            modal_particles.append(token.text.lower())

    return modal_particles

# divide dataset into 4 chunks.
chunks = np.array_split(combined_df, 4)

#save each chunk as csv files.
for i, chunk in enumerate(chunks):
    chunk.to_csv(f"combined_chunk_{i}.csv", index=False)

from tqdm.notebook import tqdm
import json

particle_sentences = []

# chunk 0
chunk_df = pd.read_csv("combined_chunk_0.csv")

for idx, row in tqdm(enumerate(chunk_df.iterrows()), total=len(chunk_df)):

    text = row[1]['Review']

    try:
        detected_particles = classify_modal_particle_bert_grammar(text)
        for particle in detected_particles:
            particle_sentences.append({
                'Review': text,
                'Particle': particle
            })
    except Exception as e:
        print(f"Error at index {idx}: {e}")
        continue

    if idx % 100 == 0:
        with open('modal_particle_progress_chunk0.json', 'w', encoding='utf-8') as f:
            json.dump(particle_sentences, f, ensure_ascii=False, indent=2)

#final save
with open('modal_particle_final_chunk0.json', 'w', encoding='utf-8') as f:
    json.dump(particle_sentences, f, ensure_ascii=False, indent=2)

from google.colab import files
files.download('modal_particle_final_chunk0.json')

df_json = pd.read_json("modal_particle_final_chunk0.json", encoding="utf-8")
df_json.head()

df_json['Particle'].value_counts()

#particle numbers in chunk 0 is enough for research

#pick 100 samples from this df.
sampled_df = df_json.groupby('Particle', group_keys=False).apply(lambda x: x.sample(n=100, random_state=42)).reset_index(drop=True)