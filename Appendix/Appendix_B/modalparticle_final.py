# -*- coding: utf-8 -*-
"""modalparticle_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18g3R6Rc-4S4MutUHBiL_Hlmpt_E-6mBm
"""

!pip install transformers
!pip install torch
!pip install pandas
!pip install matplotlib
!pip install gdown

!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz
!gunzip cc.de.300.bin.gz

!pip install fasttext

#load models.
from transformers import pipeline
import fasttext

#Multilingual BERT Model.
sentiment_model_bert = pipeline("text-classification", model="nlptown/bert-base-multilingual-uncased-sentiment")

#Huggingface BERT Model.
sentiment_model_hf = pipeline("sentiment-analysis", model="oliverguhr/german-sentiment-bert")

#FastText Model.
ft_model = fasttext.load_model('cc.de.300.bin')

import numpy as np

def get_ft_vector(text, model):
    words = text.lower().split()
    vectors = []
    for w in words:
        try:
            vectors.append(model.get_word_vector(w))
        except:
            continue
    if not vectors:
        return np.zeros(model.get_dimension())
    return np.mean(vectors, axis=0)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import pandas as pd
import gdown

#filmstarts dataset.

reviews_file_id = "17l-LzyTfB_1FlIpQDAlV2a0Yestyu-CI"
labeled_file_id = "12t-EeIEeMq1tdQRkYbP6XFohhdAHIh0o"

reviews_file_path = "reviews.csv"
labeled_file_path = "labels.csv"

gdown.download(f"https://drive.google.com/uc?id={reviews_file_id}", reviews_file_path, quiet=False)
gdown.download(f"https://drive.google.com/uc?id={labeled_file_id}", labeled_file_path, quiet=False)

reviews_df = pd.read_csv(reviews_file_path, sep='\t', header=None, names=['URL', 'Rating', 'Review'], on_bad_lines='skip')
labeled_df = pd.read_csv(labeled_file_path, sep='\t', header=None, names=['Label'])

labeled_df[['Sentiment', 'Score']] = labeled_df['Label'].str.split(' ', n=1, expand=True)
labeled_df['Score'] = pd.to_numeric(labeled_df['Score'], errors='coerce')
reviews_df = reviews_df.drop(columns=['Rating'])
combined_df = pd.concat([reviews_df, labeled_df], axis=1)
combined_df = combined_df.dropna(subset=['Review', 'Score'])
combined_df.head()

#pick samples.
sampled_df = combined_df.sample(1000, random_state=42)
sampled_df['Score'].value_counts()

balanced_df = combined_df.groupby('Score', group_keys=False).apply(lambda x: x.sample(n=200, random_state=42), include_groups=False)

texts = sampled_df['Review'].tolist()
labels = sampled_df['Score'].tolist()

#Vectorize and encode labels
X = np.array([get_ft_vector(t, ft_model) for t in texts])
le = LabelEncoder()
y = le.fit_transform(labels)

#train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

from sklearn.metrics import classification_report

#classification report for FastText model.
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred, target_names=[str(c) for c in le.classes_]))

#function for sentiment scores.

def get_sentiment_score_ft(text):
    vec = get_ft_vector(text, ft_model).reshape(1, -1)
    pred = clf.predict(vec)
    return le.inverse_transform(pred)[0]

def get_sentiment_score_hf(review):
    try:
        result = sentiment_model_hf(review[:512])
        label = result[0]['label']
        if label == 'positive':
            return 5
        elif label == 'neutral':
            return 3
        else:
            return 1
    except:
        return None

def get_sentiment_score_bert(review):
    try:
        result = sentiment_model_bert(review[:512])
        stars = result[0]['label']
        score = int(stars[0])
        return score
    except:
        return None

!python -m spacy download de_core_news_sm

from transformers import AutoTokenizer, AutoModel
import torch
import spacy
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = AutoTokenizer.from_pretrained("bert-base-german-cased")
model = AutoModel.from_pretrained("bert-base-german-cased")
nlp = spacy.load("de_core_news_sm")

#load modal particle chunk 0 retrieved from the filmstart dataset.
from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/Dataset/filmstarts/modal_particle_final_chunk0.json"

df_json = pd.read_json(file_path, encoding="utf-8")
df_json.head()

df_json['Particle'].value_counts()

particles = df_json['Particle'].unique()

sampled_rows = []

for particle in particles:
    filtered = df_json[df_json['Particle'] == particle]
    sample_n = min(100, len(filtered))
    sampled = filtered.sample(sample_n, random_state=42)
    sampled_rows.append(sampled)

df_sampled = pd.concat(sampled_rows, ignore_index=True)

df_sampled['Particle'].value_counts()

df_sampled.head()

import pandas as pd

particle_results = []

#sampling 100 sentences for each modal particles.
for particle in df_json['Particle'].unique():
    filtered_reviews = df_json[df_json['Particle'] == particle]

    sample_size = min(100, len(filtered_reviews))
    if sample_size == 0:
        continue

    sampled_reviews = filtered_reviews.sample(sample_size, random_state=42)

    hf_scores = sampled_reviews['Review'].apply(get_sentiment_score_hf)
    bert_scores = sampled_reviews['Review'].apply(get_sentiment_score_bert)
    ft_scores = sampled_reviews['Review'].apply(get_sentiment_score_ft)

    hf_mean = hf_scores.dropna().mean()
    bert_mean = bert_scores.dropna().mean()
    ft_mean = ft_scores.dropna().mean()

    particle_results.append({
        'Particle': particle,
        'HuggingFace Model Score': hf_mean,
        'BERT Multilingual Model Score': bert_mean,
        'FastText Model Score': ft_mean,
        'Sample Size': sample_size,
        'Reviews': sampled_reviews['Review'].tolist()
    })

results_df = pd.DataFrame(particle_results)
results_df

#remove modal particles from the original sentence.

def remove_particle_from_review(review, particle):
    return review.replace(particle, '').replace(particle.capitalize(), '').strip()

#finding delta

df_with_deltas = results_df.copy()

expanded_rows = []
for _, row in results_df.iterrows():
    for review in row['Reviews']:
        expanded_rows.append({
            'Particle': row['Particle'],
            'Review': review
        })

df_expanded = pd.DataFrame(expanded_rows)

# Score_b for particle removed sentences
df_with_deltas['Score_b_hf'] = df_expanded.apply(
    lambda row: get_sentiment_score_hf(remove_particle_from_review(row['Review'], row['Particle'])),
    axis=1
)

df_with_deltas['Score_b_bert'] = df_expanded.apply(
    lambda row: get_sentiment_score_bert(remove_particle_from_review(row['Review'], row['Particle'])),
    axis=1
)

df_with_deltas['Score_b_ft'] = df_expanded.apply(
    lambda row: get_sentiment_score_ft(remove_particle_from_review(row['Review'], row['Particle'])),
    axis=1
)

# delta = b-a
df_with_deltas['Delta_hf'] = df_with_deltas['Score_b_hf'] - df_with_deltas['HuggingFace Model Score']
df_with_deltas['Delta_bert'] = df_with_deltas['Score_b_bert'] - df_with_deltas['BERT Multilingual Model Score']
df_with_deltas['Delta_ft'] = df_with_deltas['Score_b_ft'] - df_with_deltas['FastText Model Score']

df_with_deltas.head()

df_with_deltas

delta_subset = df_with_deltas[['Particle', 'Delta_hf', 'Delta_bert', 'Delta_ft']]
print(delta_subset)

previous_study_delta = [0.3185, 0.0269, 0.1065, -0.1214, -0.1571, -0.0143, 0.3721, 0.1929, 0.6071]

particles = df_with_deltas['Particle'].tolist()
delta_hf = df_with_deltas['Delta_hf'].tolist()
delta_bert = df_with_deltas['Delta_bert'].tolist()
delta_ft = df_with_deltas['Delta_ft'].tolist()

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

plt.plot(particles, delta_hf, label="HuggingFace Delta", marker='o')
plt.plot(particles, delta_bert, label="BERT Multilingual Delta", marker='s')
plt.plot(particles, delta_ft, label="FastText Delta", marker='^')
plt.plot(particles, previous_study_delta, label="Previous Study Delta", marker='x', linestyle='--', color='gray')

plt.title("Delta Sentiment Score Comparison by Modal Particle")
plt.xlabel("Modal Particle")
plt.ylabel("Delta Sentiment Score")
plt.xticks(rotation=45)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()